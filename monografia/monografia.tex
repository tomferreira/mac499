\documentclass[a4paper,12pt]{article}
\usepackage[brazilian]{babel}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
\usepackage{graphicx}
\usepackage{longtable}
\usepackage{float}
\usepackage{subfigure}
\usepackage{multirow}
\usepackage{multicol}
\usepackage{verbatim}

\title{Classificação não-supervisionada hierárquica de artigos jornalísticos}
\author{Cirillo Ribeiro Ferreira \\ Orientador: Prof. Dr. Alair Pereira do Lago}
\graphicspath{{images/}}

\begin{document}
\hyphenation{i-ma-gem do-cu-men-tos di-fe-ren-tes m\'a-xi-ma a-pre-sen-ta-dos ou-tros e-li-mi-nar e-xer-c\'i-cios me-lho-ra-ram me-lho-rar ta-ma-nho co-li-ne-a-res e-li-mi-nar sa-li-ên-cias co-mer-ci-ais}
\maketitle
\newpage

\tableofcontents
\newpage
\section {Introdução}
A tarefa de classificar e agrupar documentos textuais remonta desde a antiguidade, iniciando-se na criação da primeira biblioteca do mundo em Nínive, Assíria (atual Iraque), por volta do século VII a.C \cite{redarterj}. Desde então, profissões como bibliotecário, documentalista e arquivista foram criadas para atuar na organização desses documentos que vem sendo produzidos pela humanidade.

Porém com a criação da internet e a popularização de seu uso como ferramenta de comunicação, houve uma explosão de informação que tornou praticamente impossível a classificação desses novos tipos de documentos de maneira manual.

A área de classificação de documentos é de grande interesse e possui diversas aplicações práticas como classificação de \textit{spam}, identificação do idioma utilizado e análise de sentimento. Em especial, a classificação de artigos jornalísticos oferece um enorme desafio devido à grande quantidade de novos documentos criados diariamente e a diversidade de temas abordados, especialmente em blogs, mas que carecem de melhor organização.

\subsection{Motivação}
\label{sec:motivacao}

A motivação desse trabalho vem da insatisfação com a falta de tempo para uma leitura mais crítica e ciente de um contexto maior devido à enorme quantidade de notícias que estamos sujeitos a receber todos os dias e da organização simplória feita pelos grandes portais de notícias.

O motivo de se utilizar algoritmos não-supervisionados vem da necessidade de encontrar relações implícitas entre os artigos e também da diversidade e quantidade de artigos disponíveis para classificação, uma vez que fica quase que impossível criar manualmente um conjunto de treinamento para subsídio de um algoritmo supervisionado.

Ao contrário da classificação supervisionada, a classificação não-supervisionada não necessita de um conjunto de treinamento como entrada, permitindo o seu uso em conjuntos de dados bem variados, algo que é bem comum em artigos jornalísticos.

A ideia não é separar os artigos jornalísticos em grupos muito generalistas comumente usados em jornais ou portais de notícias, como por exemplo as seções: Brasil, Mundo, Economia e Esportes; mas encontrar grupos mais intrínsecos que representem com mais acurácia a informação passada por esses artigos.

Segundo Martin Ester \textit{et al.} \cite{Ester03}, existem alguns desafios da área de agrupamento de documentos como: alta dimensionalidade da coleção, onde cada palavra distinta na coleção constitui uma dimensão e quanto maior a dimensão, maior o desafio para a construção de um algoritmo escalável e eficiente; grande volume de dados; busca de alta precisão e consistência, pois dependendo do tipo de documentos, alguns algoritmos apresentam resultados muito abaixo do esperado; e descrição mais significativa dos grupos, pois facilita a utilização pelos usuários.

\subsection{Objetivos}

O trabalho de classificação não-supervisionada dos artigos jornalísticos foi dividido em duas partes:

\begin{itemize}
%\item Estudos das principais classes de algoritmos para agrupamento de documentos textuais.
\item Criação de uma biblioteca para agrupamento de artigos jornalísticos.
\item Proposta e implementação do sistema hVINA (\textit{Hierarchical Viewer of News Articles}) para análise de agrupamento de artigos jornalísticos.
\end{itemize}

A representação hierárquica de um agrupamento de documentos é geralmente feita através de um dendrograma, como na figura \ref{fig:resultado}.

\begin{figure}[H]
    \begin{center}
        \includegraphics[scale=0.5]{resultado.png}
    \end{center}
    \caption{Esquema do resultado esperado}
    \label{fig:resultado}
\end{figure}

\subsection{Organização do trabalho}

%Na seção \ref{sec:arquitetura} é explicada a arquitetura de um sistema para análise e leitura de tablóides que seja robusto e expansível a vários tipos de tablóides. A seção \ref{sec:segmentacao} descreve a etapa de segmentação de uma página de um tablóide. A seção \ref{sec:detect_ocr} descreve a etapa de detecção de textos e uso de um programa de OCR. Na seção \ref{sec:resultados} são apresentados os resultados obtidos e na seção \ref{sec:conclusoes} são explicitadas as conclusões finais do trabalho, assim como uma avaliação do cumprimento dos objetivos propostos. Na seção \ref{sec:subj} é apresentada uma análise subjetiva do curso do Bacharelado em Ciência da Computação.

Na seção \ref{sec:fundamentos} é apresentado os fundamentos da área de reconhecimento de padrão e mais especificamente de análise de agrupamento. Na seção \ref{sec:fihc} é discutido em detalhe o algoritmo FIHC, escolhido para ser implementado na biblioteca e sistema hVINA propostos na seção \ref{sec:arquitetura_biblioteca} para o agrupamento de artigos jornalísticos. Na seção 5 são apresentados os resultados obtidos com essas ferramentas. Na seção 6 são apresentadas as conclusões finais do trabalho. E por fim, na seção 7 é apresentada uma análise subjetiva do curso do Bacharelado em Ciência da Computação.

\newpage
\section {Fundamentos}
\label {sec:fundamentos}
Antes de apresentar e discutir sobre a biblioteca desenvolvida e o sistema proposto, serão apresentados alguns conceitos básicos necessários para a compreensão do conteúdo e dos resultados obtidos.

\subsection{Reconhecimento de padrão}
\label{sec:reconhecimento_padrao}

Segundo Theodoridis e Koutroumbas  \cite{Koutroumbas06}, reconhecimento de padrão é uma área da ciência cujo objetivo é a classificação de objetos dentro de um número de categorias ou classes. Esses objetos de estudo variam de acordo com cada aplicação, podem ser imagens, sinais em forma de ondas (como voz, luz, rádio) ou qualquer tipo de medida que necessite ser classificada. As implicações práticas desses estudos permeiam todas as áreas de atuação humana, principalmente numa sociedade pós-industrial, onde a necessidade de automação das diversas atividades é essencial.  Sua aplicação vai desde sistemas que utilizam visão computacional, passando por reconhecimento de caracteres até sistemas para auxílio à tomada de decisão.

\subsubsection{Formas de processo de aprendizagem}
\label{sec:formas_processo_aprendizagem}

Na área de reconhecimento de padrão os algoritmos utilizam técnicas de aprendizagem computacional para a realização de alguma tarefa e eles são geralmente classificados de acordo a necessidade de intervenção humana durante alguma fase de aprendizagem do algoritmo. São elas: aprendizagem supervisionada,  aprendizagem não-supervisionada e aprendizagem semi-supervisionada.

A classificação supervisionada consiste na utilização de um conjunto de dados previamente classificados (ou anotados), que servirá na construção de um modelo para a classificação de novos objetos. Ou seja, algoritmos que utilizam esse conjunto de dados fazem uso de um conhecimento a priori, originado de uma fonte externa. Esse conjunto de dados inicial é denominado de conjunto de treinamento (\textit{training set}). Porém, em muitos casos a criação de um conjunto de treinamento é inviável devido à natureza do problema. Para esses tipos, é então empregado a classificação não-supervisionada, que não faz uso de um conjunto de treinamento, pois o próprio algoritmo busca descobrir similaridades intrínsecas nos objetos e agrupa aqueles objetos mais similares. \cite{Koutroumbas06, Jain99, Manning09}

Por fim, a classificação semi-supervisiona emprega técnicas da classificação supervisionada e não-supervisionada, geralmente com um conjunto pequeno de treinamento.

No contexto de reconhecimento de padrão, é comum chamar a classificação supervisionada de apenas classificação e a classificação não-supervisionada de análise de agrupamento, ou simplesmente agrupamento (do inglês \textit{clustering}). 

%% TODO: Colocar a figura
%A figura ? mostra o esquema das formas de aprendizagem.

%[Seria muito bom se colocasse um esquema dessas formas de processo de aprendizagem]

\subsection{Análise de agrupamento}
\label{sec:analise_agrupamento}

Análise de agrupamento é uma classificação de padrão que emprega o processo de aprendizagem não-supervisionada e tem como objetivo o particionamento de objetos em grupos cujos membros sejam similares entre si e diferentes dos membros de outros grupos \cite{Jain99}. Porém, também tem os objetivos secundários de evitar grupos muito pequenos ou muito grandes e encontrar grupos que façam sentido ao usuário. Segundo Theodoridis e Koutroumbas \cite{Koutroumbas06}, a análise de agrupamento é largamente utilizada na resolução de diversos problemas, e pode ser encontrada em outras áreas por nomenclaturas diferentes, como taxonomia numérica em biologia, e tipologia na área das ciências sociais.

Um grande problema enfrentado na análise de agrupamento é a dimensionalidade ou variáveis dos dados envolvidos, sejam eles observações de estrelas no céu, observações de organismos vivos na natureza ou documentos textuais disponíveis na internet, e uma técnica bastante utiliza para tentar solucionar tal problema é a redução dimensional através da extração ou seleção de características, que envolve em sintetizar os dados em características mais relevantes para o problema principal, tentando minimizar a perda de informação.

\subsection{Agrupamento de documentos textuais}
\label{sec:agrupamento_doc_textual}

Uma aplicação da análise de agrupamento é na organização de documentos. Geralmente ao fazer alguma consulta em sistemas de busca são retornadas centenas de documentos que possuem algum tipo de relevância à consulta, porém esse grande número de resultado pode inviabilizar a utilização desses sistemas, por isso a maioria deles utiliza algumas técnicas de análise de agrupamento para organizar automaticamente os resultados em categorias que fazem sentido ao usuário. 

Em especial, quando se fala em documentos textuais, o processo de redução dimensional através de extração de características dos dados é de sensível importância devido à grande redundância dos dados, que neste caso, são sentenças ou palavras. Processar os documentos sem antes usar uma boa técnica de redução dimensional acarreta quase sempre em baixa eficiência da solução.

\subsection{Tipos de algoritmo de agrupamento}
\label{sec:tipos_algoritmo_agrupamento}

Um algoritmo de agrupamento objetiva encontrar as classes naturais das observações, baseados em alguma similaridade. Existem diversos algoritmos para agrupamento desenvolvidos \cite{Koutroumbas06, Jain99}, porém neste capítulo serão citadas duas categorias de algoritmos mais empregados nesse processo. Uma explicação com aspectos mais detalhados desses métodos pode ser encontrada em \cite{Koutroumbas06, Jain99}.

\subsubsection{Agrupamento plano}
\label{sec:agrupamento_plano}

Agrupamento plano é a categoria de algoritmos cujo os grupos resultantes são dados num único nível, onde nenhum grupo tem relação com outro. O principal algoritmo dessa categoria é o \textit{k-means} \cite{Manning09}.

Antes de falar do \textit{k-means} é preciso relembrar um ponto essencial ao entendimento desse algoritmo, que é a definição de centroide. Na geometria, o centroide é o ponto central de uma figura. Dessa forma, dado um grupo, se ligarmos com um segmento de linha as observações mais distantes podemos visualizar uma figura geométrica, e o centroide desse grupo seria a observação mais próxima ao centro dessa figura.

%%[Inserir algoritmo k-means]

Em termos gerais, o objetivo do \textit{k-means} é dividir n observações em \textit{k} grupos, minimizando a média das distâncias euclidianas quadráticas entre as observações e os respectivos centroides dos grupos, ou seja, cada observação é associada ao grupo cujo o centroide está mais próximo. 

O algoritmo inicia com uma escolha aleatória dos \textit{k} centroides e a cada iteração é feito um refinamento na escolha desses centroides até que não haja mais variação na escolha dos centroides ou até que se atinja um número predefinido de iterações. A figura \ref{fig:passos_kmeans} exemplifica a sequência descrita acima.

Há duas propriedades importantes do \textit{k-means}:

\begin{itemize}
\item Cada objeto deve pertencer ao menos um dos \textit{k} grupos.
\item Nenhum objeto pertence a mais que um grupo.
\end{itemize}

\begin{figure}[H]
    \begin{center}
        \includegraphics[scale=0.65]{k_means_algorithm.png}
    \end{center}
    \caption{Sequência de passos do algoritmo k-means, retirado de \cite{experiment_clustering}}
    \label{fig:passos_kmeans}
\end{figure}

Um cuidado especial na utilização desse algoritmo é na escolha do número de grupos desejado. Uma escolha inadequada poderá resultar em um mau agrupamento, unindo duas classes naturais em um único grupo ou dividindo uma classe natural em dois grupos, porém algumas técnicas tem sido desenvolvidas para resolver esse problema \cite{Jain99, Manning09}. Um exemplo de problema que pode ocorrer está ilustrado pela figura \ref{fig:problema_kmeans}.

\begin{figure}[H]
    \begin{center}
        \includegraphics[scale=0.8]{cluster_1.png}
    \end{center}
    \caption{Em (a) é mostrado um conjunto de dados fictício com três classes naturais. Em (b) quanto é escolhido o número de grupos k = 2, as classes naturais 2 e 3 são unidas num único grupo. Em (c), com k = 3, os grupos refletem as classes naturais dos dados, e em (d) com k = 4 a classe natural 3 é dividida entre dois grupos.}
    \label{fig:problema_kmeans}
\end{figure}

Conforme John Wang \cite{Wang09}, o algoritmo \textit{k-means} não é adequado para descobrir grupos de tamanhos que variam muito, um cenário comum em agrupamento de documentos. Além disso, é sensível a ruídos que pode ter uma grande influência sobre a escolha do centroide de um grupo, que por sua vez diminui a precisão do agrupamento. Todavia algoritmos como o \textit{k-medoids} \cite{Wang09} foram propostos para resolver o problema do ruído.

\subsubsection{Agrupamento hierárquico}
\label{sec:agrupamento_hierarquico}

Agrupamento plano geralmente é mais simples e fácil de implementar, porém existem certos tipos de problemas em que a visualização de um agrupamento de um único plano não é suficientemente útil, além disso, como visto no tópico anterior, esse tipo de agrupamento tem dois problemas que são a escolha do número de grupo na entrada e o fato de não serem determinísticos \cite{Manning09}. Dessa forma, algoritmos que criassem agrupamentos com vários níveis e que tivessem como requisito opcional a entrada do número de grupos foram desenvolvidos. Esses algoritmos criam uma árvore hierárquica de grupos, uma estrutura que gera mais informação, uma vez que as relações implícitas entre os grupos ficam mais evidentes.

Existem duas abordagens no funcionamento de um algoritmo hierárquico que são:

\begin{itemize}
\item Abordagem aglomerativa:. Gera a árvore de grupo ao continuamente calcular a similaridade de todos os pares de grupos e unificar o par mais similar (segundo um critério de similaridade), criando a árvore das folhas até a raiz, por isso é também conhecida como abordagem bottom-up. Um exemplo dessa abordagem é ilustrado na figura \ref{fig:hierarchical_algorithm}.

\item Abordagem divisiva: Gera a árvore de grupo da raiz às folhas (top-down), utilizando em cada nível da árvore um algoritmo de agrupamento plano para a divisão dos grupos em nós filhos.
\end{itemize}

\begin{figure}[H]
    \begin{center}
        \includegraphics[scale=0.5]{hierarchical_algorithm.png}
    \end{center}
    \caption{Exemplo de um algoritmo hierárquico aglomerativo, retirado de \cite{Segaran08}, onde o critério de similaridade utilizado é a distância euclidiana.}
    \label{fig:hierarchical_algorithm}
\end{figure}

\subsection{Critério de avaliação}
\label{sec:criterio_avaliação}

Existem 2 tipos de erros que podem ocorrer no processo de agrupamento. O primeiro, chamado de falso positivo (FP) ou erro tipo I, ocorre quando o algoritmo associa dois documentos não similares a um mesmo grupo, e o segundo, chamado de falso negativo (FN) ou erro tipo II, ocorre quando o algoritmo associa dois documentos similares a grupos distintos. Um critério de avaliação justo tem de levar em conta as ocorrências desses dois erros. A tabela \ref{table:matrix_contingencia} a seguir, mostra a matriz de contingência entre os documentos e grupos.

\begin{table}[h]
\centering
\begin{tabular}{ | p{3cm} | p{4cm} | p{4cm} | }
\hline
& Mesmo grupo & Diferentes grupos                                                                   \\ \hline
Documentos similares	& Verdadeiro positivo (VP) & Falso negativo (FN) ou Erro tipo II \\ \hline
Documentos não similares	&  Falso positivo (FP) ou Erro tipo I & Verdadeiro negativo (VN) \\
\hline
\end{tabular}
\caption{Matriz de contingência}
\label{table:matrix_contingencia}
\end{table}

Duas medidas importantes na área de reconhecimento de padrão e também na estatística, são a precisão e cobertura do resultado observado. A precisão mede a fração de documentos que estão classificados corretamente no grupo e é dado pela seguinte fórmula:

\begin{equation} P = \frac{TP} {TP + FP} \end{equation}

Já a cobertura mede a fração de documentos que foram classificados corretamente dentre os documentos similares e é dado pela fórmula:

\begin{equation} R = \frac{TP} {TP + FN} \end{equation}

Por exemplo, seja dado o conjunto de documentos \{1, 2, 3, 4, 5, 6\}, onde $N_{1}$ = \{1, 3, 5\} fazem parte de uma classe natural, ou seja, são similares; e $N_{2}$ = \{2, 4, 6\} fazem parte de outra classe natural. Dado um grupo C = \{1, 3, 4, 5\} a precisão de C em relação a $N_{1}$ é P = 3/4 e a cobertura é R = 3/3 = 1, ou seja, nem todos os documentos de C são similares entre si, apesar de que todos os documentos de $N_{1}$ foram agrupados corretamente em C.

Porém, a utilização de duas medidas para avaliação e comparação de algoritmos torna difícil determinar se um algoritmo é superior a outro ou não. Pois é melhor um algoritmo com alta precisão e baixa cobertura, ou vice-versa? A resposta para essa pergunta depende do contexto onde é aplicado o agrupamento. Segundo Manning et al, na maioria das vezes é mais aceitável o erro de tipo I, pois separar documentos similares em grupos distintos é geralmente pior do que juntar documentos não similares num mesmo grupo \cite{Ester03}.

Dado isto, um critério para avaliação bastante utilizado é a medida F (\textit{F-measure}), pois avalia o agrupamento utilizando a média harmônica ponderada da precisão e da cobertura. A medida F é dada por:

\begin{equation} F_{\beta} = (1 + \beta^2) \times \frac{P \times R} {\beta^2 P + R} \end{equation}
onde $\beta^2$ $\in$ [0, $\infty$].

Um valor para $\beta$ bastante utilizado é $\beta$ = 1, pois tanto a precisão quanto a cobertura terão o mesmo peso na medida. Neste caso, a medida seria simplificada da seguinte forma:

\begin{equation} F_{1} = 2 \times \frac{P \times R} {P + R} \end{equation}

Já definindo $\beta$ > 1, a precisão será mais enfatizada em relação à cobertura. Note que para a medida F, o algoritmo não basta ter apenas boa precisão ou apenas boa cobertura, mas sim ambas características, o que a torna um dos critérios mais utilizados para a análise. Porém, ela não é o único critério de avaliação existente. Dentro da área de estudo sobre avaliação de agrupamentos existem critérios internos e externos e a medida F se enquadra na segunda categoria, pois avalia o agrupamento a partir de uma classificação de referência \cite{Ester03}.

\newpage
\section {FIHC}
\label {sec:fihc}

\textit{Frequent Itemset-based Hierarchical Clustering} (FIHC) é um algoritmo para agrupamento hierárquico de documentos proposto por Martin Ester \textit{et al.} \cite{Ester03} e que utiliza o conceito de conjuntos de itens frequentes (do inglês \textit{frequent itemset}).

Segundo Martin Ester \textit{et al.}, a ideia utilizada para o critério de agrupamento dos documentos é que existe algum conjunto de itens frequentes para cada grupo (tópico) no conjunto de documentos, e diferentes grupos compartilham poucos conjuntos de itens frequentes.

Segundo os autores, o algoritmo tenta resolver alguns desafios da área como: alta dimensionalidade da coleção, onde cada palavra distinta na coleção constitui uma dimensão, e quanto maior a dimensão, maior o desafio para a construção de um algoritmo escalável e eficiente; grande volume de dados; alta precisão e consistência, dependendo do tipo de documentos, alguns algoritmos apresentam resultados muito abaixo do esperado; e descrição mais significativa dos grupos, pois facilita a utilização pelos usuários.

Um ponto de interesse é que, diferentemente dos algoritmos mais comuns para agrupamento hierárquico, no FIHC a parametrização do número de grupos desejado é opcional, assim o usuário não precisa ter nenhum conhecimento prévio da coleção de documentos.

O algoritmo utiliza a abordagem aglomerativa, descrita na seção 2.4.2, porém ao invés de construir a árvore baseando-se na similaridade entre as observações (documentos) ele a faz baseando-se nos grupos \cite{Ester03}. Possui dois passos principais, que é a construção dos grupos e posteriormente a criação da árvore hierárquica, entretanto antes de apresentá-los é necessário introduzir algumas definições.

\subsection {O que é conjunto de itens frequentes}
\label {sec:conjunto_itens_frequentes}

Por definição, conjunto de itens é o conjunto de palavras que ocorrem em conjunto na coleção de documentos. Logo conjunto de itens frequentes é o conjunto de palavras que ocorrem numa quantidade de documentos acima de um limiar de suporte definido. Em outras palavras, suponha que \emph{l} seja o limiar de suporte e que F seja um conjunto de palavras. O suporte (\textit{support} em inglês) de F é a proporção de documentos no qual F é um subconjunto das palavras desses documentos. Assim, dizemos que F é um conjunto de itens frequentes se o seu suporte é \emph{l} ou superior. A tabela \ref{table:colecao_artigos} ilustra um exemplo:

\begin{table}[H]
\centering
\begin{tabular}{ | l | p{10cm} | }
\hline
Documento & Palavras (itens)                                                                   \\ \hline
doc.1     & \{Bolsa, europeia, opera, em, queda, após, anúncio, de, pacote, grego\}            \\ \hline
doc.2     & \{Japão, e, Grécia, ficam, no, 0, a, 0, no terceiro, dia, de, competição\}         \\ \hline
doc.3     & \{Descoberta, de, tumba, misteriosa, anima, Grécia, em, meio, à, crise, europeia\} \\ \hline
doc.4     & \{Grécia, assume, presidência, da, União, Europeia, por, seis, meses\}             \\ \hline
doc.5     &  \{Crise, está, prejudicando, saúde, mental, dos, gregos\} \\
\hline
\end{tabular}
\caption{Coleção de artigos}
\label{table:colecao_artigos}
\end{table}

No exemplo, a palavra “Grécia” aparece nos documentos, (2), (3) e (4), assim seu suporte é 3; a palavra “crise” aparece em (3) e (5), logo seu suporte é 2; a palavra “de” ocorre em todos os documentos, exceto em (4) e (5), assim seu suporte é 3; a palavra “europeia” ocorre em (1), (3) e (4), logo seu suporte é 3. Todas as outras palavras ocorrem apenas em um único documento. Logo se for definido o limiar de suporte \emph{l} = 2, teremos como conjuntos de itens frequentes os conjuntos \{Grécia\}, \{crise\}, \{de\} e \{europeia\}. Além disso, as duplas \{Grécia, europeia\}, \{Grécia, de\} e \{europeia, de\} ocorrem em 2 documentos, logo também são conjuntos de itens frequentes.

No caso do algoritmo FIHC, conjuntos de itens frequentes são também denominados de conjuntos globais de itens frequentes, e suporte é denominado de \textit{global support}.

\subsubsection {Algoritmo Apriori}
\label {sec:algoritmo_apriori}

A extração dos conjuntos de itens frequentes é uma das técnicas mais utilizadas para caracterização de dados com objetivo de resumir os atributos gerais mais relevantes de um conjunto de dados. O exemplo mostrado na seção anterior é uma forma de encontrar os conjuntos de itens frequentes, porém esse método não é eficiente quando precisa ser utilizado em uma coleção grande de documentos. Um dos algoritmos mais populares para tal propósito, seja em um banco de dados ou em documentos textuais, é o algoritmo Apriori \cite{Agrawal94}.

O algoritmo utiliza a propriedade de anti-monotonicidade \cite{Rajaraman11}, onde se um conjunto de itens não é frequente, então todos os seus superconjuntos também não serão.

Ele inicia a busca a partir de uma coleção de conjunto de itens frequentes com cardinalidade \emph{k} = 1, chamado de $F_{1}$ . A partir daí todos os conjuntos de cardinalidades maiores são obtidos a partir de uma coleção de candidatos C. O algoritmo para quando não encontrar mais nenhum conjunto de itens frequentes. % A figura ? mostra o algoritmo:

\begin{algorithm}
\caption{Algoritmo Apriori para extração de conjuntos de itens frequentes em documentos textuais}
\label{algorithm_apriori}
\begin{algorithmic}[1]
\State $F_{1} \gets (\text{Conjunto de itens frequentes de cardinalidade 1})$
\State $k \gets 1$

\While {$F_{k} \neq \emptyset$}
	\State $C_{k+1} \gets candidate\_gen(F_{k}) $
	
	\ForAll {documento d na coleção}
		\State $C'_{d} \gets subset(C_{k+1},d)$
		
		\ForAll {candidato c $\in$ $C'_{d}$}
			\State $c.count \gets c.count + 1$
		\EndFor
		
	\EndFor
	
	\State $F_{k+1} \gets \{ c \in C_{k+1} | c.count \geq \text{suporte mínimo} \}$
	
	\State $k \gets k + 1$
\EndWhile

\Return $\cup_{k} F_{k}$
\end{algorithmic}
\end{algorithm}

\subsection {O que é \textit{cluster frequent item} e \textit{cluster support}}
\label {sec:cluster_frequent_item_e_cluster_support}

Dado o grupo $C$, um elemento de um conjunto global de itens frequentes é denominado de \textit{cluster frequent item} de $C$ se este elemento está contido em um número mínimo de documentos de $C$, definido como \textit{minimum cluster support}.

Além disso, \textit{cluster support} de um item em $C$ é definido como a porcentagem de documentos de $C$ que contém o item.

\subsection {O que é tf-idf}
\label {sec:tf_idf}

Tf-idf é uma medida estatística usada para avaliar o quão importante uma palavra é para um documento em relação a uma coleção de documentos. A ideia é que se uma palavra é muito comum em diferentes documentos da coleção, então provavelmente ela tem pouca relevância, como por exemplo, a conjunção aditiva “e”. A importância aumenta proporcionalmente ao número de vezes que uma palavra aparece no documento, mas é compensado pela frequência da palavra no corpus. O tf-idf é dado pelo produto de duas medidas: a frequência do termo e a frequência inversa do documento, dado por:

\begin{equation} \mbox{tf-idf}(t, d) = \mbox{tf}(t, d) \times \log \frac{N} {\mbox{df}(t)} \end{equation}
onde $\mbox{tf}(t,d)$ é a razão entre o número de vezes que o termo $t$ aparece no documento $d$ e a quantidade de termos em $d$; $N$ é a quantidade de documentos na coleção e $\mbox{df}(t)$ é a quantidade de documentos da coleção que possui o termo $t$. Existem diversas variantes dessa medida como visto em \cite{Manning09}.

\subsection {Construção dos grupos}
\label {sec:construcao_grupos}

Apresentado alguns conceitos relacionados ao FIHC, vamos ao algoritmo propriamente dito. Como já mencionado, o primeiro passo do algoritmo é construir os grupos, e isso é feito em duas etapas. Primeiramente, os grupos iniciais são construídos a partir dos conjuntos globais de itens frequentes extraídos da coleção pelo algoritmo Apriori e então é feito o processo de disjunção desses grupos para que ao final, um documento pertença somente a um único grupo.

Na criação dos grupos iniciais, para cada conjunto global de itens frequentes é criado um grupo que inicialmente incluirá todos os documentos que contêm o conjunto de itens em seu corpo. Logo, podemos considerar que cada grupo possuirá um conjunto de itens frequentes gerador. 

Note que esses grupos podem não ser disjuntos, pois um documento pode conter vários conjuntos de itens frequentes.

\begin{table}[h]
\centering
\begin{tabular}{ | l | l | l | l | l | l | }
\hline
Documento & mercado & crise & universo & Grécia & inteligência \\ \hline
artigo.1     & 0 & 0 & 3 & 0 & 2 \\ \hline
artigo.2     & 2 & 0 & 0 & 1 & 0 \\ \hline
artigo.3     & 1 & 4 & 0 & 3 & 0 \\ \hline
artigo.4     & 0 & 0 & 4 & 0 & 1 \\ \hline
artigo.5     & 0 & 1 & 0 & 0 & 2 \\ \hline
artigo.6     & 3 & 2 & 0 & 0 & 0 \\ \hline
artigo.7     & 1 & 0 & 0 & 2 & 2 \\ \hline
artigo.8     & 0 & 1 & 0 & 0 & 0 \\ \hline
artigo.9     & 0 & 0 & 2 & 0 & 1 \\ \hline
artigo.10   & 0 & 0 & 3 & 0 & 0 \\
\hline
\end{tabular}
\caption{Exemplo de uma coleção de documentos e os respectivos valores tf-idf de cada palavra em relação a cada documento}
\label{table:exemplo_docs}
\end{table}

\begin{table}[h]
\centering
\begin{tabular}{ | l | l | }
\hline
Conjunto global de itens frequentes & \textit{Global support} \\ \hline
\{mercado\}                         & 40\% \\ \hline
\{crise\}                               & 40\% \\ \hline
\{universo\}                         & 40\% \\ \hline
\{Grécia\}                             & 30\% \\ \hline
\{inteligência\}                     & 50\% \\ \hline
\{mercado, Grécia\}             & 30\% \\ \hline
\{universo, inteligência\}     & 30\% \\
\hline
\end{tabular}
\caption{Conjuto global de itens frequentes da coleção de exemplo da tabela \ref{table:exemplo_docs}. O \textit{minimum global support} é definido como 30\%. A relação completa dos conjuntos de itens da coleção pode ser encontrada no anexo A.}
\label{table:conjunto_itens_frequentes}
\end{table}

\subsubsection {Disjunção dos grupos sobrepostos}
\label {sec:disjuncao_grupos_sobrepostos}

A disjunção é um processo importante, pois garante que ao final do algoritmo cada documento pertença somente ao grupo que melhor o descreva. Para cada documento D, é listado todos os grupos no qual ele pertence, e após isso é calculado para cada um desses grupo C uma medida que indicará a “relevância” de D em relação a C. Essa medida tem o nome de Score e é proposto em \cite{Ester03}.

Após os cálculos da medida Score de D para cada grupo C, é mantido o documento somente no grupo que maximizou essa medida. Esse cálculo de relevância de um grupo inicial $C_{i}$ para um documento $D_{j}$ é definido por:

\begin{equation}
\begin{split}
  Score(C_{i} \gets D_{j}) = |\sum_{x}tf-idf(x, D_{j}) * cluster\_support(x)| \\
  - |\sum_{x'}tf-idf(x', D_{j}) * global\_support(x')|
\end{split}
\end{equation}
onde $x$ representa um \textit{global frequent item} que está contido em $D_{j}$ e também é um \textit{cluster frequent item} de $C_{i}$; $x’$ representa um \textit{global frequent item} que está contido em $D_{j}$, mas que não é um \textit{cluster frequent item} de $C_{i}$; $tf-idf(x, D_{j})$ e $tf-idf(x’, D_{j})$ são as medidas tf-idf dos itens $x$ e $x’$ em $D_{j}$; $cluster\_support(x)$ é o \textit{cluster support} de $x$ e $global\_support(x’)$ é o \textit{global support} de $x’$.

\begin{table}[h]
\centering
\begin{tabular}{ | l | p{4cm} | p{4.5cm} | }
\hline
Grupo & Documentos do grupo & \textit{Cluster frequent items} e \textit{cluster support} \\ \hline
C(mercado)			& artigo.2, artigo.3, artigo.6 e artigo.7 & (mercado/cs = 100\%), (Grécia/cs = 75\%), (crise/cs = 50\%) \\ \hline
C(crise)                         	& artigo.3, artigo.5, artigo.6 e artigo.8 & (crise/cs = 100\%), (mercado/cs = 100\%)  \\ \hline
C(universo)			& artigo.1, artigo.4, artigo.9 e artigo.10 & (universo/cs = 100\%), (inteligência/cs = 75\%)  \\ \hline
C(Grécia)			& artigo.2, artigo.3 e artigo.7 & (Grécia/cs = 100\%), (mercado/cs = 100\%)  \\ \hline
C(inteligência)		& artigo.1, artigo.4, artigo.5, artigo.7 e artigo.9 & (inteligência/cs = 100\%), (universo/cs = 60\%)  \\ \hline
C(mercado, Grécia)		& artigo.2, artigo.3 e artigo.7 & (mercado/cs = 100\%), (Grécia/cs = 100\%)  \\ \hline
C(universo, inteligência)	& artigo.1, artigo.4 e artigo.9 & (universo/cs = 100\%), (inteligência/cs = 100\%)  \\
\hline
\end{tabular}
\caption{Grupos criados a partir dos conjuntos globais de itens frequentes da tabela \ref{table:conjunto_itens_frequentes}, onde o \textit{minimum cluster support} = 50\%.}
\label{table:grupos}
\end{table}

O primeiro termo da função $Score(C_{i} \gets D_{j})$ recompensa $C_{i}$ se o \textit{global frequent item} $x$ em $D_{j}$ é também um \textit{cluster frequent item} de $C_{i}$ e o segundo termo penaliza $C_{i}$ se o \textit{global frequent item} $x’$ em $D_{j}$ não é um \textit{cluster frequent item} de $C_{i}$. De acordo com Martin Ester \textit{et al.} \cite{Ester03}, intuitivamente, um grupo $C_{i}$ é “bom” para um documento $D_{j}$ se há muitos \textit{global frequent items} em $D_{j}$ que aparecem em “muitos” documentos em $C_{i}$. Se há mais de um grupo que maximiza a medida $Score(C_{i} \gets D_{j})$, então é escolhido o grupo com maior número de itens em seu conjunto de itens frequentes gerador.

Seguindo o exemplo da tabela \ref{table:grupos}, note que o documento artigo.3 pertence aos grupos C(mercado), C(crise), C(Grécia) e C(mercado, crise). Para se decidir em qual desses grupos o artigo deve permanecer, são calculados os seguintes $Score$:

\begin{flushleft}

$Score(C(mercado) \gets artigo.3) = 1 * 1 + 4 * 0,5 + 3 * 0,75 = 5,25$

$Score(C(crise) \gets artigo.3) = 1 * 0,5 + 4 * 1 - 3 * 0,3 = 3,6$

$Score(C(Gr\acute{e}cia) \gets artigo.3) = 1 * 1 + 3 * 1 - 4 * 0,4 = 2,4$

$Score(C(mercado, Gr\acute{e}cia) \gets artigo.3) = 1 * 1 + 3 * 1 - 4 * 0,4 = 2,4$

\end{flushleft}

Desta maneira, o documento artigo.3 seria mantido apenas no grupo C(mercado).

\begin{table}[h]
\centering
\begin{tabular}{ | l | p{4cm} | p{4.5cm} | }
\hline
Grupo & Documentos do grupo & \textit{Cluster frequent items} e \textit{cluster support} \\ \hline
C(mercado)			& artigo.3 e artigo.6 & (mercado/cs = 100\%), (Grécia/cs = 75\%), (crise/cs = 50\%) \\ \hline
C(crise)                         	& artigo.8 & (crise/cs = 100\%), (mercado/cs = 100\%)  \\ \hline
C(universo)			& artigo.10 & (universo/cs = 100\%), (inteligência/cs = 75\%)  \\ \hline
C(Grécia)			& & nenhum  \\ \hline
C(inteligência)		& artigo.5 & (inteligência/cs = 100\%), (universo/cs = 60\%)  \\ \hline
C(mercado, Grécia)		& artigo.2 e artigo.7 & (mercado/cs = 100\%), (Grécia/cs = 100\%)  \\ \hline
C(universo, inteligência)	& artigo.1, artigo.4 e artigo.9 & (universo/cs = 100\%), (inteligência/cs = 100\%)  \\
\hline
\end{tabular}
\caption{Grupos disjuntos}
\label{table:grupos_disjuntos}
\end{table}

\subsection {Criação da árvore de grupos}
\label {sec:criacao_arvore_grupos}

Uma vez computado os grupos iniciais, podemos visualizar cada um deles como um tópico ou subtópico na coleção de documentos. Agora é necessário criar a árvore, colocando os tópicos mais generalistas no topo e os mais específicos abaixo. Essa árvore tem como objetivos criar uma estrutura para a navegação entre os documentos e também facilitar a poda dos grupos muito específicos.

Os grupos são inicialmente aninhados na árvore de acordo com a quantidade de itens em seu conjunto global gerador. Dessa forma, aqueles que possuem somente um item, estão no primeiro nível da árvore, e assim por diante. A raiz (nível 0) da árvore é por definição um grupo vazio e sem nenhum item em seu conjunto global de itens frequentes. Como dito no início desse capítulo, o FIHC utiliza a abordagem aglomerativa para a criação da hierarquia, ou seja, para cada grupo C no nível \emph{k} é escolhido um pai dentre os potenciais grupos do nível \emph{k-1}, cujo o conjunto global seja subconjunto dos conjuntos globais de itens frequentes de C. Se houver mais de um potencial pai no nível \emph{k-1}, então é escolhido aquele grupo que maximiza a medida \emph{Score}, similar ao processo descrito na seção \ref{sec:disjuncao_grupos_sobrepostos}. Isso é melhor detalhado em \cite{Ester03}.

Uma vez escolhido um grupo pai para cada grupo da coleção, é necessário verificar se é possível unificar os grupos similares ou com tópicos muito específicos, garantindo uma hierarquia mais natural para a navegação e melhorando a acurácia do algoritmo. Isso é feito de duas formas: 1) através do processo de poda dos grupos filhos e 2) unificação dos grupos similares do nível 1.

A medida de similaridade utilizada no algoritmo FIHC utiliza a ideia de tratar um grupo como um documento conceitual, definindo a partir da combinação de todos os documentos do grupo, e a partir daí é tirado a medida através de um cálculo bem similar à medida $Score$ apresentada na seção anterior. Dado um grupo $C_{a}$ e $C_{b}$, a medida de similaridade entre os grupos é calculado a partir da média geométrica entre a similaridade de $C_{a}$ para $C_{b}$ e da similaridade de $C_{b}$ para $C_{a}$. Da seguinte forma:

\begin{equation}
  Inter\_Sim(C_{a} \leftrightarrow C_{b}) = [Sim(C_{a} \gets C_{b}) * Sim(C_{b} \gets C_{a})]^\frac{1} {2}
\end{equation}
onde a similaridade $Sim$ de $C_{a}$ para $C_{b}$ é dado como:

\begin{equation}
  Sim(C_{a} \gets C_{b}) = \frac{ Score(C_{a} \gets doc(C_{b})) }{ \sum_{x}\mbox{tf-idf}(x,doc(C_{b})) + \sum_{x'}\mbox{tf-idf}(x',doc(C_{b})) } + 1
\end{equation}

Em $Sim(C_{i} \gets C_{j})$, a medida $Score$ é a mesma discutida na seção anterior, porém a única diferença é que o documento $doc(C_{j})$ utilizado no cálculo é um documento conceitual, construído a partir da combinação de todos os documentos da subárvore de $C_{j}$, e $\mbox{tf-idf}(x, doc(C_{b}))$ e $\mbox{tf-idf}(x’, doc(C_{b}))$ são, respectivamente, as medidas tf-idf dos itens $x$ e $x’$ em $doc(C_{j})$.

\subsection {Rotulagem do grupo}
\label {sec:rotulagem_grupo}

Uma etapa importante no processo de agrupamento é a de rotulagem dos grupos, pois para que um usuário entenda o resultado do algoritmo e o utilize sem dificuldades é necessário que a rotulagem seja legível e faça sentido a ele. 

No caso do FIHC, a própria abordagem de criação dos grupos a partir dos conjuntos globais de itens frequentes da coleção e posterior escolha dos \textit{cluster frequent items} torna natural pensar que esses conjuntos de palavras também são bons candidatos para dar nome aos próprios grupos. 

% TODO: Colocar exemplo
%[Exemplo]

\newpage

\section {Arquitetura da biblioteca e sistema hVINA}
\label {sec:arquitetura_biblioteca}

A biblioteca proposta neste trabalho tem como objetivo tratar todo os passos de uma solução para agrupamento, desde o pré-processamento dos documentos até a implementação do algoritmo de agrupamento de fato. Inicialmente foi implementado o algoritmo FIHC, porém sua arquitetura é modular e permite a substituição por outros algoritmos.

Já o sistema tem como objetivo apresentar o agrupamento dos artigos jornalísticos de forma simples e intuitiva para os usuários, onde as configurações dos parâmetros do algoritmo FIHC seja pouco necessária por eles, uma vez que a crescente utilização da internet como meio de divulgação de artigos pelos jornais e revistas torna necessário um mecanismo automático e não-supervisionado para extração e organização do conhecimento, buscando encontrar entre os artigos relacionamentos a princípio escondidos. A figura \ref{fig:arquitetura_biblioteca} mostra o processo para o agrupamento dos artigos.

\begin{figure}[h]
    \begin{center}
        \includegraphics[scale=0.35]{arquitetura.png}
    \end{center}
    \caption{Sequência de passos da biblioteca proposta.}
    \label{fig:arquitetura_biblioteca}
\end{figure}

A biblioteca detecta e trata artigos em português, inglês e espanhol, porém permite a extensão para outros idiomas, bastando escolher um algoritmo para tokenização e criar uma lista de \textit{stopwords} específicos do idioma. Além disso, a biblioteca é modular, permitindo a troca dos algoritmos utilizados em cada processo de forma simples e sem a necessidade de uma mudança geral no sistema.

Os processos de detecção de idioma, tokenização, remoção dos \textit{stopwords} e \textit{stemming} utilizam técnicas e algoritmos já bem conhecidos e empregados na área de reconhecimento de padrões. 

O sistema hVINA foi desenvolvido utilizando o \textit{framework} Rails \cite{rails} e outras tecnologias Web, como a marcação HTML, Javascript e CSS3. Todo o código-fonte segue junto a esta monografia.

\subsection {Detecção de idioma}
\label {sec:deteccao_idioma}

O processo de detecção do idioma da coleção de documentos é parte essencial em um sistema de agrupamento, uma vez que algoritmos empregados nos passos seguintes, como o de tokenização e remoção dos \textit{stopwords}, requerem o conhecimento do idioma utilizado nos documentos.

\subsubsection {Método de n-grama}
\label {sec:metodo_ n_grama}

A tarefa de encontrar o idioma de um documento é em si própria um problema na área de reconhecimento de padrão, visto que é preciso classificar o documento em um conjunto de classes (idiomas). Um algoritmo bastante conhecido para essa tarefa baseia-se nas frequências dos n-gramas do documento. Esse algoritmo utiliza o processo de aprendizagem supervisionado, pois cria o modelo dos n-gramas mais frequentes de cada idioma a partir de um conjunto de textos de treinamento.

\begin{figure}[h]
    \begin{center}
        \includegraphics[scale=0.4]{n_grama.png}
    \end{center}
    \caption{Esquema do método n-grama, retirado de \cite{Trenkle94}. O perfil à esquerda representa o modelo n-grama do idioma inglês, obtido a partir de um conjunto de textos para treinamento, o perfil à direita representa os n-gramas mais frequentes de um documento de exemplo e a última coluna indica a distância entre a posição desses n-grama em relação ao perfil do idioma.}
    \label{fig:metodo_n_grama}
\end{figure}

\subsection {Tokenização}
\label {sec:tokenizacao}

Como um dos pontos centrais do algoritmo FIHC é a utilização dos conjuntos de itens frequentes da coleção de documentos, uma ação importante para que a biblioteca funcione com boa acurácia é estabelecer um processo que encontre nos textos todos os termos que o compõe. Isso se dá pelo processo de tokenização ou segmentação de texto. De acordo com Manning \textit{et al.} \cite{Manning09}, tokenização é o processo de quebra do documento em partes, chamados de \textit{tokens}. A definição de \textit{token} varia de acordo com o contexto do problema, e na prática pode ser palavras, frases ou símbolos. No caso deste trabalho, os \textit{tokens} são encontrados à medida que um espaço em branco ou uma marcação gráfica aparece entre eles, exceto o hífen. É necessário falar também que neste trabalho as palavras \textit{token}, termo e item se equivalem. Veja a figura \ref{fig:tokenizacao} para um exemplo ilustrativo desse processo:

\begin{figure}[H]
    \begin{center}
        \includegraphics[scale=0.6]{tokenizacao.png}
    \end{center}
    \caption{Exemplo do processo de tokenização em um  trecho de artigo.}
    \label{fig:tokenizacao}
\end{figure}

Um bom algoritmo para extração dos tokens deve levar em consideração o idioma do texto, pois em geral o processo de tokenização é difícil, devido a construções de palavras que podem surgir durante o processo, como as contrações de palavras inglesas, como \textit{“we’ve many time (...)”}, ou as palavras compostas em português como “latino-americano”. 

O algoritmo de tokenização implementado inicialmente na biblioteca é o \textit{Treebank tokenizer} \cite{treebank}, porém a extensão para outros algoritmos de tokenização é possível devido à modularidade da biblioteca.

\subsection {Limpeza}
\label {sec:limpeza}

Um atributo requerido para um sistema de agrupamento de documentos é a escolha de grupos (tópicos) significativos, porém nem sempre o algoritmo de agrupamento sozinho é capaz de determinar a representatividade de uma determinada palavra no momento da construção dos grupos. Por esse motivo, é usado a técnica de limpeza para eliminação das palavras que são extremamente comuns no idioma desses documentos. Essas palavras são chamadas de \textit{stopwords}. Uma estratégia simples para determinar a lista delas é ordená-las pela frequência que aparecem na coleção, porém geralmente essa lista é composta pelas palavras de classes gramaticais como preposição, artigo, advérbio, numeral, pronome e pontuações em geral.

No caso deste trabalho é feito também a eliminação das marcações gráficas que não fazem parte de uma palavra propriamente dita.

Continuando com o exemplo da figura \ref{fig:limpeza}, com a remoção dos \textit{stopwords} o resultado seria da seguinte forma:

\begin{figure}[H]
    \begin{center}
        \includegraphics[scale=0.6]{limpeza.png}
    \end{center}
    \caption{Exemplo do processo de limpeza no conjunto de palavras resultante na figura \ref{fig:tokenizacao}.}
    \label{fig:limpeza}
\end{figure}

\subsection {\textit{Stemming}}
\label {sec:stemming}

\textit{Stemming} é o processo de unificar as formas variantes de uma palavra em uma representação comum, chamado de \textit{stem} \cite{Orengo01}. Por exemplo, as palavras “corredor”, “corrida” e “correria” podem ser reduzidas para a representação “corr”.

Esse processo é essencial para um sistema de agrupamento, pois, assim como o processo de limpeza, tem o objetivo de aumentar a eficiência do algoritmo de agrupamento.

\subsubsection {Tipos de erros}
\label {sec:tipos_erros}

Geralmente os diversos algoritmos de \textit{stemming} são analisados através de dois tipos de erros:

\begin{itemize}
\item \textit{Overstemming}: quando é retirado da palavra um sufixo grande o suficiente para que palavras com significados diferentes sejam unificadas numa única representação. Por exemplo, quando as palavras “escritor” e “escrutínio” são reduzidas para “escr”.

\item \textit{Understemming}: quando é retirado da palavra um sufixo suficientemente pequeno para que palavras com o mesmo significado sejam separadas em representações distintas. Por exemplo, quando as palavras “alimentar” e “alimentação” são reduzidas para “alimenta” e “alimentaç”, respectivamente.
\end{itemize}

\subsubsection {O algoritmo RSLP}
\label {sec:algoritmo_RSLP}

A maioria dos algoritmos para \textit{stemming} baseia-se no método de remoção de afixos (sufixos e prefixos), definidos a partir de um conjunto de regras predefinidas. Caso a palavra contenha o prefixo ou sufixo definido por alguma regra, ele é removido \cite{Alexandre07}.

O problema desse tipo de método é que esses algoritmos são muito dependentes da linguagem para os quais foram criados, assim é difícil a criação de um algoritmo universal.

No caso da língua inglesa, o algoritmo mais usado é o de Porter \cite{Manning09}, já no caso da língua portuguesa o algoritmo escolhido para este estudo é o Removedor de Sufixos da Língua Portuguesa (RSLP), devido ao seu bom desempenho \cite{Orengo01}.

\begin{figure}[H]
    \begin{center}
        \includegraphics[scale=0.5]{RSLP.png}
    \end{center}
    \caption{Sequência de passos do algoritmo RSLP, retirado de \cite{Orengo01}.}
    \label{fig:rslp}
\end{figure}

Um exemplo de regra utilizada no algoritmo RSLP é:

\begin{equation}
  \text{“is”, 2, “il”, \{ “lápis", “cais”, “mais”, “crúcis”, “pois”, “dois”, “leis” \}}
\end{equation}

Essa regra é utilizada para redução de plural nos casos onde a palavra termina com o sufixo “is”. Assim “is” é o sufixo a ser substituído, 2 é o tamanho mínimo para o \textit{stem} final, “il” é o novo valor do sufixo, e as palavras “lápis”, “cais”, “mais”, “crúcis”, “pois”, “dois” e “leis” são as exceções da regra. 

\subsection {Agrupamento}
\label {sec:agrupamento}

Até então os processos discorridos neste capítulo fazem parte da etapa de pré-processamento da coleção de documentos. Aqui começa de fato o processo principal da biblioteca, que é o de agrupar os artigos jornalísticos. Como visto na seção 2.4.2, existem diversos algoritmos para agrupamento hierárquico desenvolvidos na área de análise de agrupamento. 

No primeiro momento foi implementado o algoritmo FIHC, estudado no capítulo 3. Porém será visto na próxima seção que a implementação de outros algoritmos é simples.

\subsection {Interface gráfica}
\label {sec:interface_grafica}

Com o intuito de demostrar a utilização da biblioteca, esse trabalho também inclui a criação do sistema hVINA, acrônimo para \textit{Hierarchical Viewer of News Articles}. Com esse sistema é possível avaliar a viabilidade da biblioteca de agrupamento de artigos jornalísticos com alguns dos maiores portais de publicações do meio digital. A interface inicial do sistema está ilustrado pela figura \ref{fig:hvina_inicial} de onde é possível escolher alguma das publicações pré-selecionadas, ou então importar uma coleção de artigos jornalísticos de interesse do usuário.

\begin{figure}[H]
    \begin{center}
        \includegraphics[scale=0.3]{hvina_inicial.png}
    \end{center}
    \caption{Tela inicial do sistema hVINA}
    \label{fig:hvina_inicial}
\end{figure}

O hVINA também permite a configuração do algoritmo de agrupamento, no caso em especial o FIHC, de forma simples.

\begin{figure}[H]
    \begin{center}
        \includegraphics[scale=0.45]{hvina_configuracao.png}
    \end{center}
    \caption{Tela de configuração do algoritmo de agrupamento.}
    \label{fig:hvina_configuracao}
\end{figure}

\begin{figure}[H]
    \begin{center}
        \includegraphics[scale=0.35]{hvina_clustering.png}
    \end{center}
    \caption{Tela para visualização do agrupamento.}
    \label{fig:hvina_clustering}
\end{figure}

\section {Resultados}
\label {sec:resultados}

\subsection {A biblioteca}
\label {sec:a_biblioteca}

A biblioteca foi desenhada utilizando alguns padrões de projeto, dentre eles o \emph{Strategy Pattern} para encapsular os algoritmos de agrupamentos em classes independentes e o \emph{Template Method Pattern} para auxiliar na definição das classes desses algoritmos numa estrutura comum, permitindo que novos algoritmos sejam implementados ou substituídos sem a necessidade de alteração na estrutura da biblioteca.

Desde sua concepção, ela teve como requisito ser modular, o que permitirá a inclusão de outros algoritmos de agrupamento de forma simples e sem a necessidade de alteração dos módulos e classes principais da biblioteca. O projeto está disponível no repositório online Github, no endereço https://github.com/tomferreira/hierarchical\_clustering.

A biblioteca foi desenvolvida em Ruby e segue a especificação do \emph{RubyGem} e por esse motivo pode ser facilmente reutilizada em outros projetos Ruby. Sua principal classe é a HierarchicalClustering, de onde é controlado o fluxo de dados entre os passos da biblioteca. A figura \ref{fig:exemplo_biblioteca} ilustra a utilização da biblioteca, adotando o algoritmo FIHC para o agrupamento e imprimindo o resultado num formato aceito pelo sistema hVINA.

\begin{figure}[H]
    \begin{center}
        \includegraphics[scale=0.6]{exemplo_biblioteca.png}
    \end{center}
    \caption{Exemplo de utilização da biblioteca com o algoritmo FIHC.}
    \label{fig:exemplo_biblioteca}
\end{figure}

Outro ponto importante é que a coleção dos artigos deve seguir um padrão e conter alguns dados obrigatórios como o título, conteúdo e URL do artigo. Cada artigo deve estar separado em arquivo diferentes e os dados dispostos no formato JSON como mostrado no exemplo da figura \ref{fig:exemplo_artigo}.

\begin{figure}[H]
    \begin{center}
        \includegraphics[scale=0.6]{exemplo_artigo.png}
    \end{center}
    \caption{Exemplo de um documento aceito pela biblioteca.}
    \label{fig:exemplo_artigo}
\end{figure}

A formatação do resultado do agrupamento também pode ser estendida, bastando criar uma classe que deverá herdar a classe Clustering::Fihc::OutputManager e informá-la ao algoritmo utilizado. A figura \ref{fig:exemplo_formatacao} ilustra um exemplo.

\begin{figure}[H]
    \begin{center}
        \includegraphics[scale=0.6]{exemplo_formatacao.png}
    \end{center}
    \caption{Exemplo de extensão da formatação do resultado de um agrupamento.}
    \label{fig:exemplo_formatacao}
\end{figure}

\subsection {O sistema hVINA}
\label {sec:o_sistema_hvina}

Apesar do intuito inicial do sistema hVINA ser um protótipo para a demonstração prática da biblioteca desenvolvida, ele é totalmente funcional e pode ter mais funcionalidades acrescentadas. 

O sistema foi desenvolvido em Ruby juntamente com o \emph{framework} Rails, e seu código-fonte também está disponível no repositório Github, no endereço https://github.com/tomferreira/hVINA.

%Falar mais

\newpage

\section {Conclusão}
\label {sec:conclusao}

Esse trabalho discutiu a análise de agrupamento de documentos textuais com ênfase em coleções de artigos jornalísticos, além de estudar algumas classes de algoritmos que podem ser usadas para tal propósito.

O objetivo principal foi fazer essa análise a partir da criação de uma biblioteca que abrangesse todas as etapas de uma solução para agrupamento, desde o pré-processamento da coleção até a execução do algoritmo de agrupamento em si, tendo como requisito o fato de que não fosse necessário nenhum conhecimento prévio da coleção por parte do usuário.

Inicialmente foi implementado na biblioteca o algoritmo de agrupamento \textit{Frequent Itemset-based Hierarchical Clustering} (FIHC), visto que o agrupamento hierárquico é uma forma mais natural para a navegação nos grupos (tópicos) e permite reconhecer relações implícitas entre esses grupos de maneira muito mais simples.

Além da biblioteca, foi desenvolvido também o sistema \textit{Hierarchical Viewer of News Articles} (hVINA) para ser uma interface gráfica dela, permitindo a escolha da coleção e a visualização do agrupamento resultante de forma muito mais intuitiva aos usuários finais.

O resultado obtido pela biblioteca em conjunto com o sistema hVINA demostra a viabilidade de uma solução que tenha boa usabilidade e seja amigável a qualquer usuário - mesmo aqueles não técnicos, onde não haja a necessidade de treinamento do algoritmo e que sua configuração seja quase zero.

\subsection {Considerações finais}
\label {sec:consideracoes_finais}

Segundo o professor Clay Shirky, da New York University, o problema da \textit{overdose} de informação, da qual estamos sujeitos diariamente, está no fato de não sabermos filtrar o que é importante para nós \cite{shirky}. Logo, uma possível solução é justamente criar mecanismos automáticos que possam organizar essas informações de modo a nos ajudar a decidir o que é importante ou não para nós.

Agrupar grande quantidade de artigos de forma que os grupos sejam intuitivos ao usuário, isto é, que possuam rótulos significativos, não é uma tarefa fácil, visto que a diversidade de tópicos possíveis é imensa. A solução apresentada neste trabalho é somente uma das possíveis abordagens, e novas tentativas de resolver esse problema devem também ser empenhadas com outras técnicas.

\subsection {Análise subjetiva}
\label {sec:analise_subjetiva}

\subsubsection {Desafios e frustrações encontradas}
\label {sec:desafios_frustracoes_encontradas}

Durante o início do desenvolvimento do trabalho, houveram diversos desafios quanto à definição do que seria feito. A área de reconhecimento de padrão é, em particular, muito empolgante e com imensas aplicações práticas. Porém, encontrar algum problema que fosse, ao mesmo tempo, capaz de ser debruçado e “solucionado” em apenas 1 ano (menos ainda se considerarmos que tive que realizar diversas tarefas não propriamente ditas de análise ou desenvolvimento) e também útil e relevante, não foi simples.

Uma vez fechado o escopo - e dado um título mais detalhado do meu trabalho, as demais tarefas começaram a caminhar bem. 

Conciliar esse trabalho com outras matérias do curso e demandas de outra ordem, como emprego e vida pessoal, não foi nada simples. Mas agora no final vejo que assim como o conhecimento técnico adquirido na área de reconhecimento de padrão, adquiri também um grande amadurecimento quanto à organização de todas essas tarefas.

Já nas questões técnicas, houveram algumas frustrações durante o transcorrer do trabalho, como a falta de material acadêmico mais farto de algoritmos que tratasse em específico de agrupamento de documentos textuais.

Infelizmente só foi implementado um único algoritmo de agrupamento - o FIHC. Outros algoritmos estavam em vista, porém alguns problemas na implementação do FIHC adiaram as demais implementações.

O maior problema técnico do projeto no entanto é o desempenho do algoritmo FIHC, que ainda precisaria de muito trabalho para chegar a um tempo aceitável (quase tempo real).

\subsubsection {Disciplinas relevantes para o trabalho}
\label {sec:disciplinas relevantes para o trabalho}

As disciplinas mais relevantes durante o desenvolvimento deste trabalho foram:
\begin{enumerate}
    \item \textbf{Armazenamento e Recuperação de Informação (MAC0333):} Essa disciplina introduziu os principais conceitos da área de recuperação de informação, como tf-idf e medida F.

    \item \textbf{Engenharia de software (MAC0332):} Durante todo o processo de desenvolvimento e gerenciamento das atividades envolvidas no processo de criação da biblioteca e do sistema foram empregados técnicas apresentadas nesta disciplina.

    \item \textbf{Programação orientada a objeto (MAC0441) e Laboratório de programação II (MAC0242):} Tanto a biblioteca quanto o sistema utilizam o paradigma de programação orientada à objeto. Em especial foi em Laboratório de programação que pela primeira vez tive em contato com o desenvolvimento de um grande projeto de software.

    \item \textbf{Estrutura de dados (MAC0323):} Muitas das estruturas de dados do algoritmos de agrupamento foram baseados ou emprestados dos conceitos apresentado nesta disciplina.

    \item \textbf{Introdução à computação (MAC0110) e Princípios de Desenvolvimento de Algoritmos (MAC0122):} Essas disciplinas foram a base necessária para um bom aprendizado de todo o conceito teórico e todas as outras disciplinas.
\end {enumerate}

\subsubsection {Trabalhos futuros}
\label {sec:trabalhos_futuros}

Melhorar o desempenho do algoritmo de agrupamento FIHC é um trabalho que pode ser continuado, e técnicas como a programação distribuída podem contribuir para essa melhoria.

Além disso, implementar a detecção de outros idiomas na biblioteca é algo que naturalmente precisará ser feito. E novos gráficos para a visualização desses grupos podem ser utilizados para complementar o já existente, enriquecendo ainda mais a experiência do usuário com o sistema.

Outra possibilidade é integrar a biblioteca ao projeto DocumentCloud (http://www.documentcloud.org/public/search/).

\subsubsection {Agradecimentos}
\label {sec:agradecimentos}

Agradeço a todos os professores do curso de Ciências da Computação, em especial ao Alair Pereira do Lago que muito me ajudou durante o desenvolvimento desse trabalho, fazendo sempre comentários muito valiosos para mim. Por fim, agradeço aos meus pais e minha irmã por todo apoio e paciência durante o ano todo.

\newpage

\section {Anexo A}
\label {sec:anexo_A}

\begin{longtable}{ | l | l | }
\hline
Conjunto de itens & \textit{Global support} \\ \hline
\{mercado\}				& 40\% \\ \hline
\{crise\}				& 40\% \\ \hline
\{universo\}				& 40\% \\ \hline
\{Grécia\}				& 30\% \\ \hline
\{inteligência\}			& 50\% \\ \hline
\{mercado, crise\}			& 20\% \\ \hline
\{mercado, universo\}		& 0\% \\ \hline
\{mercado, Grécia\}		& 30\% \\ \hline
\{mercado, inteligência\}		& 10\% \\ \hline
\{crise, universo\}			& 0\% \\ \hline
\{crise, Grécia\}			& 10\% \\ \hline
\{crise, inteligência\}		& 10\% \\ \hline
\{universo, Grécia\}		& 0\% \\ \hline
\{universo, inteligência\}		& 30\% \\ \hline
\{Grécia, inteligência\}		& 10\% \\ \hline
\{mercado, crise, universo\}		& 0\% \\ \hline
\{mercado, crise, Grécia\}			& 10\% \\ \hline
\{mercado, crise, inteligência\}		& 0\% \\ \hline
\{mercado, universo, Grécia\}		& 0\% \\ \hline
\{mercado, universo, inteligência\}	& 0\% \\ \hline
\{mercado, Grécia, inteligência\}		& 10\% \\ \hline
\{crise, universo, Grécia\}			& 0\% \\ \hline
\{crise, universo, inteligência\}		& 0\% \\ \hline
\{crise, Grécia, inteligência\}		& 0\% \\ \hline
\{universo, Grécia, inteligência\}		& 0\% \\ \hline
\{mercado, crise, universo, Grécia\}			& 0\% \\ \hline
\{mercado, crise, universo, inteligência\}		& 0\% \\ \hline
\{mercado, crise, Grécia, inteligência\}			& 0\% \\ \hline
\{mercado, universo, Grécia, inteligência\}		& 0\% \\ \hline
\{crise, universo, Grécia, inteligência\}			& 0\% \\ \hline
\{mercado, crise, universo, Grécia, inteligência\}	& 0\% \\
\hline
\caption{Relação completa dos conjuntos de itens da coleção de documentos da tabela \ref{table:exemplo_docs}}
\label{table:colecao_completa_conjutos_itens_frequentes}
\end{longtable}

\newpage

\begin{thebibliography}{9}

\bibitem{redarterj}
    http://redarterj.wordpress.com/2009/10/15/a-primeira-biblioteca-do-mundo

\bibitem{Koutroumbas06}
    Sergios Theodoridis, Konstantinos Koutroumbas.
    \emph{Pattern Recognition}.
    Elsevier; 3 edition,
    2006.
    
\bibitem{Jain99}
    Jain, A.K., Murty, M.N., Flynn, P.J.
    \emph{Data Clustering: A Review}.
    ACM Computing Surveys 31 (3), 264–322,
    1999.
    
\bibitem{Manning09}
    Manning, Christopher D.; Raghavan, Prabhakar; Schütze, Hinrich.
    \emph{An Introduction to Information Retrieval}.
    Cambridge: Cambridge University Press, 569p,
    2009.
    
\bibitem{Orengo01}
    Viviane Orengo, Cristian Huyck.
    \emph{A Stemming Algorithm for the Portuguese Language}.
    Proceedings of the Eighth International Symposium on String Processing and Information Retrieval,
    2001.

\bibitem{Alexandre07}
    Alexandre Ramos Coelho.
    \emph{Stemming para a lingua portuguesa: Estudo, análise e melhoria do algoritmo RSLP}.
    2007.
    
\bibitem{Ester03}
    Benjamim C. M. Fung, Ke Wang, Martin Ester.
    \emph{Hierarchical Document Clustering Using Frequent Itemsets}.
    2003.
    
\bibitem{Segaran08}
    Toby Segaran.
    \emph{Programming collective intelligence : building smart Web 2.0 applications}.
    O’Reilly, 1 edition,
    2008.
    
\bibitem{Wang09}
    John Wang.
    \emph{Encyclopedia of Data Warehousing and Mining}.
    IGI Global Snippet, 2 edition,
    2009.
    
\bibitem{Rajaraman11}
    Jure Leskovec, Anand Rajaraman, Jeffrey D. Ullman.
    \emph{Mining of Massive Datasets}.
    Cambridge University Press, 2 edition,
    2011.
    
\bibitem{Trenkle94}
    William B. Cavnar, John M. Trenkle.
    \emph{N-Gram-Based Text Categorization}.
    In Proceedings of SDAIR-94, 3rd Annual Symposium on Document Analysis and Information Retrieval,
    1994.
    
\bibitem{Agrawal94}
    R. Agrawal and R. Srikant.
    \emph{Fast algorithms for mining association rules in large databases}.
    Proceedings of the 20th International Conference on Very Large Data Bases, VLDB, pages 487-499, Santiago, Chile,
    September, 1994.

\bibitem{treebank}
    http://www.nltk.org/\_modules/nltk/tokenize/treebank.html

\bibitem{experiment_clustering}
    http://www.dormantroot.com/2014/02/my-experiment-with-clustering.html

\bibitem{rails}
    http://rubyonrails.org/
    
\bibitem{shirky}
    Web 2.0 Expo NY: Clay Shirky. It's Not Information Overload. It's Filter Failure.
    http://blip.tv/web2expo/web-2-0-expo-ny-clay-shirky-shirky-com-it-s-not-information-overload-it-s-filter-failure-1283699

\end{thebibliography}
\end{document}
​
